---
title: "Computational Modeling - Week 5 - Assignment 2 - Part 2"
author: "Riccardo Fusaroli"
date: "2/19/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(ggplot2, rethinking)
```

## In this assignment we learn how to assess rates from a binomial distribution, using the case of assessing your teachers' knowledge of CogSci.

### Second part: Focusing on predictions

Last year you assessed the teachers (darned time runs quick!). Now you want to re-test them and assess whether your models are producing reliable predictions. In Methods 3 we learned how to do machine-learning style assessment of predictions (e.g. rmse on testing datasets). Bayesian stats makes things a bit more complicated. So we'll try out how that works.


Questions to be answered (but see guidance below):
1- Write a paragraph discussing how assessment of prediction performance is different in Bayesian vs. frequentist models
**When asessing the predictions of the model in frequentist statistics we look at the residuals and the RMSE (average error/distance predicted by the model to the actual data. The standard deviation of the residuals). Meanwhile in Bayesian statistics we look at the predictive posterior which is made up from samples from the posterior. The samples are values randomly drawn from the parameter values (a vector), and the probability of each value is therefore given by the posterior. As a result our predictive posterior gives us a probability distribution of the different values, predicting the likelihood of each value. So while frequentist statistics gives us a specific prediction based on our model, Bayesian statistics gives us a probability distribution that tells us something about the likelihood of each value**

2- Provide at least one plot and one written line discussing prediction errors for each of the teachers.

This is the old data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh:   160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel:  66 correct answers out of 132 questions

This is the new data:
- Riccardo: 9 correct answers out of 10 questions (then he freaks out about teaching preparation and leaves)
- Kristian: 8 correct answers out of 12 questions
- Josh:   148 correct answers out of 172 questions (again, Josh never gets bored)
- Mikkel:  34 correct answers out of 65 questions

Guidance Tips

1. There are at least two ways of assessing predictions.

2. Last year's results are this year's expectations. **<- tip for assessing predictions one of the ways. old posterior as new prior**

3. Are the parameter estimates changing? (way 1)
**old model new data**
**parameter estimate is the posterior. look at the posterior of the model describing data from last year. take tip 2, run new model on new data and see whether posterior moves**

4. How does the new data look in last year's predictive posterior? (way 2)
**plot the predictive posterior from last years model and see how does the actual score that the teachers get look like in this. region of high or low probability.**

**In Bayesian statistics, you never throw away data. Given what we know, are our new study confirming or changing?**

#way 2: old posterior as new prior
```{r}

#Riccardo
#prior
dens = 20
p_grid = seq(from = 0, to = 1, length.out = dens)
prior = rep(1, dens)
#1 because we are agnostic and don't know anything. assume each point is equally likely.

#likelihood
R_likelihood = dbinom(3, size = 6, prob = p_grid)
#3 is how many 1's (correct answers). 6 is how big is your data (how many possible 1's. 

#then we calculate the posterior
R_unstd.posterior = R_likelihood * prior

#standardize the posterior so it sums to 1
R_posterior = R_unstd.posterior/sum(R_unstd.posterior)

#draw the plot
R_Data=data.frame(grid=p_grid,posterior=R_posterior,prior=prior,likelihood=R_likelihood)
ggplot(R_Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior/dens),color='red')+  xlab("Knowledge of CogSci")+ ylab("posterior probability") 


#Riccardo
#prior2
prior2 = R_posterior
#old posterior in new model

#likelihood
R_likelihood2 = dbinom(9, size = 10, prob = p_grid)
 

#then we calculate the posterior
R_unstd.posterior2 = R_likelihood2 * prior2

#standardize the posterior so it sums to 1
R_posterior2 = R_unstd.posterior2/sum(R_unstd.posterior2)

#draw the plot
R_Data=data.frame(grid=p_grid,posterior=R_posterior2,prior=prior2,likelihood=R_likelihood)
ggplot(R_Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior),color='red')+  xlab("Knowledge of CogSci")+ ylab("posterior probability") 
```
We observe that the parameter estimates for Riccardo's CogSci knowledge are changing, but we accredit this change both due to the prior (our previous posterior) and to the new data. 

```{r}

#Kristian
#prior
dens = 20
p_grid = seq(from = 0, to = 1, length.out = dens)
prior = rep(1, dens)
#1 because we are agnostic and don't know anything. assume each point is equally likely.

#likelihood
K_likelihood = dbinom(2, size = 2, prob = p_grid)
#3 is how many 1's (correct answers). 6 is how big is your data (how many possible 1's. 

#then we calculate the posterior
K_unstd.posterior = K_likelihood * prior

#standardize the posterior so it sums to 1
K_posterior = K_unstd.posterior/sum(K_unstd.posterior)

#draw the plot
K_Data=data.frame(grid=p_grid,posterior=K_posterior,prior=prior,likelihood=K_likelihood)
ggplot(K_Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior/dens),color='red')+  xlab("Knowledge of CogSci")+ ylab("posterior probability") 


#Kristian
#prior2
prior2 = K_posterior
#old posterior in new model

#likelihood
K_likelihood2 = dbinom(8, size = 12, prob = p_grid)
 

#then we calculate the posterior
K_unstd.posterior2 = K_likelihood2 * prior2

#standardize the posterior so it sums to 1
K_posterior2 = K_unstd.posterior2/sum(K_unstd.posterior2)

#draw the plot
K_Data=data.frame(grid=p_grid,posterior=K_posterior2,prior=prior2,likelihood=K_likelihood)
ggplot(K_Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior),color='red')+  xlab("Knowledge of CogSci")+ ylab("posterior probability") 
```
We observe that the parameter estimates for Kristian's CogSci knowledge are very different than from the prior/previous posterior. We see this change due to a combination of the prior and the new data. The prior was based on only two questions, and therefore the new data affects the posterior substantially. 

```{r}

#Josh
#prior
dens = 20
p_grid = seq(from = 0, to = 1, length.out = dens)
prior = rep(1, dens)
#1 because we are agnostic and don't know anything. assume each point is equally likely.

#likelihood
J_likelihood = dbinom(160, size = 198, prob = p_grid)
#3 is how many 1's (correct answers). 6 is how big is your data (how many possible 1's. 

#then we calculate the posterior
J_unstd.posterior = J_likelihood * prior

#standardize the posterior so it sums to 1
J_posterior = J_unstd.posterior/sum(J_unstd.posterior)

#draw the plot
J_Data=data.frame(grid=p_grid,posterior=J_posterior,prior=prior,likelihood=J_likelihood)
ggplot(J_Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior/dens),color='red')+  xlab("Knowledge of CogSci")+ ylab("posterior probability") 


#Josh
#prior2
prior2 = J_posterior
#old posterior in new model

#likelihood
J_likelihood2 = dbinom(148, size = 172, prob = p_grid)
 

#then we calculate the posterior
J_unstd.posterior2 = J_likelihood2 * prior2

#standardize the posterior so it sums to 1
J_posterior2 = J_unstd.posterior2/sum(J_unstd.posterior2)

#draw the plot
J_Data=data.frame(grid=p_grid,posterior=J_posterior2,prior=prior2,likelihood=J_likelihood)
ggplot(J_Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior),color='red')+  xlab("Knowledge of CogSci")+ ylab("posterior probability") 
```
We observe that the parameter estimates for Josh's CogSci knowledge are similar to the previous posterior/prior. As Josh has answered a large amount of questions, the prior probably doesn't influence the posterior a lot, but because the amount of correctly answered questions are similar to the previous data, the parameter estimates look alike. 

```{r}

#Mikkel
#prior
dens = 20
p_grid = seq(from = 0, to = 1, length.out = dens)
prior = rep(1, dens)
#1 because we are agnostic and don't know anything. assume each point is equally likely.

#likelihood
M_likelihood = dbinom(66, size = 132, prob = p_grid)
#3 is how many 1's (correct answers). 6 is how big is your data (how many possible 1's. 

#then we calculate the posterior
M_unstd.posterior = M_likelihood * prior

#standardize the posterior so it sums to 1
M_posterior = M_unstd.posterior/sum(M_unstd.posterior)

#draw the plot
M_Data=data.frame(grid=p_grid,posterior=M_posterior,prior=prior,likelihood=M_likelihood)
ggplot(M_Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior/dens),color='red')+  xlab("Knowledge of CogSci")+ ylab("posterior probability") 


#Mikkel
#prior2
prior2 = M_posterior
#old posterior in new model

#likelihood
M_likelihood2 = dbinom(35, size = 64, prob = p_grid)
 

#then we calculate the posterior
M_unstd.posterior2 = M_likelihood2 * prior2

#standardize the posterior so it sums to 1
M_posterior2 = M_unstd.posterior2/sum(M_unstd.posterior2)

#draw the plot
M_Data=data.frame(grid=p_grid,posterior=M_posterior2,prior=prior2,likelihood=M_likelihood)
ggplot(M_Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior),color='red')+  xlab("Knowledge of CogSci")+ ylab("posterior probability") 
```
The parameter estimates have not changed much, but we are more certain of Mikkel's knowledge of cogsci as compared to the previous posterior/prior where it was more affected by the uniform prior as compared to now, where the prior is much more aligned with the data - creating a more certain posterior. (not sure about this)


#predictive posterior
```{r}
#Riccardo
samples = sample(p_grid, prob = R_posterior, size = 1e4, replace = TRUE)
plot(samples)
dens(samples)

#make random binomial distribution based  on samples
c = rbinom(1e4, size = 10, prob = samples)
table(c)/1e4

#make histogram of predictive posterior
simplehist(c, xlab = "Predicted correct answers based on samples")
```
The histogram shows that the predictive posterior predicts the highest probability/frequency for Riccardo answering 5/10 questions correctly. From the table we can see that the probability of Riccardo answering 5/10 questions correctly is only ~15%, which is reflected in the fact that he only answered 6 questions. 
However, in reality he answered 9/10 questions correctly, which is not reflected in the predictive posterior, and so the predictive performance of the old posterior is not very good.

```{r}
#Kristian
samples = sample(p_grid, prob = K_posterior, size = 1e4, replace = TRUE)
plot(samples)
dens(samples)

#make random binomial distribution based  on samples
c = rbinom(1e4, size = 12, prob = samples)
table(c)/1e4

#make histogram of predictive posterior
simplehist(c, xlab = "Predicted correct answers based on samples")
```
The histogram shows that the predictive posterior predicts the highest probability/frequency for Kristian answering 12/12 questions correctly. From the table we can see that the probability of Kristian answering 12/12 questions correctly is ~27%, which is reflected in the fact that although he answered 2/2 questions correct, the amount of questions influences the probability of having all questions correct again negatively. 
In reality he answered 8/12 questions correctly, which is not reflected in the predictive posterior, and so the predictive performance of the old posterior is not very good.

```{r}
#Josh
samples = sample(p_grid, prob = J_posterior, size = 1e4, replace = TRUE)
plot(samples)
dens(samples)

#make random binomial distribution based  on samples
c = rbinom(1e4, size = 172, prob = samples)
table(c)/1e4

#make histogram of predictive posterior
simplehist(c, xlab = "Predicted correct answers based on samples")
```
The histogram shows that the predictive posterior predicts the highest probability/frequency for Josh answering 138/172.From the table we can see that the probability of Josh answering 138/172 questions correctly is only ~5%, which is reflected in the fact that there are several frequencies that are possible according to the predictive posterior (ranging from 109 to 164 correct answers). 
However, in reality he answered 148/172 questions correctly, which is not reflected in the predictive posterior (about half as likely), and so the predictive performance of the old posterior is not very good.

```{r}
#Mikkel
samples = sample(p_grid, prob = M_posterior, size = 1e4, replace = TRUE)
plot(samples)
dens(samples)

#make random binomial distribution based  on samples
c = rbinom(1e4, size = 65, prob = samples)
table(c)/1e4

#make histogram of predictive posterior
simplehist(c, xlab = "Predicted correct answers based on samples")

# Another way to describe the distribution is by using HIGHEST POSTERIOR DENSITY INTERVAL (HPDI), this is the narrowest interval containing the specified probability mass. 
HPDI(samples, prob = 0.5)
# 0.4736842 0.5263158
```
The histogram shows that the predictive posterior predicts the highest probability/frequency for Mikkel answering 33/65 questions correctly. From the table we can see that the probability of Mikkel answering 32/65 questions correctly is only ~8%, which is reflected in the fact that there are several frequencies that are possible according to the predictive posterior (ranging from 14 to 50 correct answers).
However, in reality he answered 34/65 questions correctly, which is fairly close to the highest probability/frequency in the predictive posterior, and so the predictive performance of the old posterior is quite good.
